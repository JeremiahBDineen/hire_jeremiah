<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" /><meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>us_county_data</title><link rel="stylesheet" href="styles.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/default.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
  <script>hljs.highlightAll();</script>
  <style>
    pre {
      background-color: #f4f4f4;
      padding: 1em;
      border-radius: 6px;
      overflow-x: auto;
      box-shadow: 0 2px 6px rgba(0,0,0,0.1);
    }
  </style>
</head>
<body>
<header><nav>
    <ul class="navbar">
      <li><a href="index.html">Home</a></li>
      <li><a href="about.html">About Me</a></li>
      <li class="dropdown"><a href="eua_population.html" class="dropbtn">Exploratory Univariate Analysis</a>
        <div class="dropdown-content">
          <a href="eua_population.html">Population</a>
          <a href="eua_median_income.html">Median Income</a>
          <a href="eua_mean_income.html">Mean Income</a>
          <a href="eua_house_price.html">House Price</a>
          <a href="eua_home_affordability.html">Home Affordability</a>
        </div>
      </li>
      <li class="dropdown"><a href="ima_median_income_vs_household_count.html" class="dropbtn">Inferential Multivariate Analysis</a>
        <div class="dropdown-content">
          <a href="ima_median_income_vs_household_count.html">association - median income + household count</a>
          <a href="ima_house_price_vs_median_income.html">association - house price + median income</a>
        </div>
      </li>
      <li><a href="tableau_map.html">Tableau Map</a></li>
      <li class="dropdown"><a href="code_us_county_data.html" class="dropbtn">Code</a>
        <div class="dropdown-content">
          <a href="code_us_county_data.html">us_county_data</a>
          <a href="code_tigris_data.html">tigris_data</a>
          <a href="code_eua_population.html">eua_population</a>
          <a href="code_eua_median_income.html">eua_median_income</a>
          <a href="code_eua_mean_income.html">eua_mean_income</a>
          <a href="code_eua_house_price.html">eua_house_price</a>
          <a href="code_eua_home_affordability.html">eua_home_affordability</a>
          <a href="code_ima_median_income_vs_household_count.html">ima_median_income_vs_household_count</a>
          <a href="code_ima_house_price_vs_median_income.html">ima_house_price_vs_median_income</a>
        </div>
      </li>
      <li><a href="cv.html">CV</a></li>
    </ul>
  </nav></header>
<main class="page-content">
  <h1>us_county_data</h1>
  <pre><code class="language-python">
import pandas as pd
import geopandas as gpd
import requests
import re
from pathlib import Path

DOWN_DIR = Path("/Users/jeremiah/prgm/r/us_county/data")
UP_DIR = Path("/Users/jeremiah/prgm/py/us_county/data")
TIGER_DIR = UP_DIR / "tiger"
ACS_DIR = DOWN_DIR / "productDownload_2025-05-10T200912"
ZILLOW_FILE = DOWN_DIR / "County_zhvi_uc_sfrcondo_tier_0.33_0.67_sm_sa_month.csv"
YEARS = list(range(2010, 2024))

'''
https://data.census.gov/table/ACSST5Y2023.S1901?q=income&g=010XX00US$0500000 S1901 
2023 dollars
https://www.zillow.com/research/data/

Zillow Home Value Index (ZHVI): A measure of the typical home value and market changes across a given region and housing type. It reflects the typical value for homes in the 35th to 65th percentile range. Available as a smoothed, seasonally adjusted measure and as a raw measure.
Zillow publishes top-tier ZHVI ($, typical value for homes within the 65th to 95th percentile range for a given region) and bottom-tier ZHVI ($, typical value for homes within the 5th to 35th percentile range for a given region).

Zillow also publishes ZHVI for all single-family residences ($, typical value for all single-family homes in a given region), for condo/coops ($), for all homes with 1, 2, 3, 4 and 5+ bedrooms ($).

Note: Starting with the January 2023 data release, and for all subsequent releases, the full ZHVI time series has been upgraded to harness the power of the neural Zestimate.

More information about what ZHVI is and how it’s calculated is available on this overview page. Here’s a handy ZHVI User Guide for information about properly citing and making calculations with this metric.
'''

# ----------------------------------------------------------------------------
tiger_frames = []
for year in range(2013, 2024):
    filename = f"tl_{year}_us_county.zip"
    url = f"https://www2.census.gov/geo/tiger/TIGER{year}/COUNTY/{filename}"
    local_path = TIGER_DIR / filename

    if not local_path.exists():
        print(f"Downloading {filename}...")
        response = requests.get(url)
        if response.ok and "zip" in response.headers.get("Content-Type", ""):
            with open(local_path, "wb") as f:
                f.write(response.content)
            print(f"✅ Saved to: {local_path}")
        else:
            print(f"⚠️ Skipping {year} due to download issue.")
            continue

    try:
        print(f"Reading shapefile for {year}...")
        shapefile_path = f"zip://{local_path.resolve()}!tl_{year}_us_county.shp"
        gdf = gpd.read_file(shapefile_path)
        gdf["data_year"] = str(year)
        tiger_frames.append(gdf[["GEOID", "data_year", "NAME", "STATEFP", "geometry"]])
    except Exception as e:
        print(f"⚠️ Failed to read {year}: {e}")

tiger = pd.concat(tiger_frames, ignore_index=True)
tiger["GEOID"] = tiger["GEOID"].str[-5:].str.zfill(5)

# ----------------------------------------------------------------------------
acs_frames = []
for year in YEARS:
    file = ACS_DIR / f"ACSST5Y{year}.S1901-Data.csv"
    df = pd.read_csv(file, dtype=str)
    df["data_year"] = str(year)
    df = df[[
        "data_year", "GEO_ID", "NAME",
        "S1901_C01_001E", "S1901_C01_012E", "S1901_C01_013E",
        "S1901_C02_001E", "S1901_C02_012E", "S1901_C02_013E",
        "S1901_C03_001E", "S1901_C03_012E", "S1901_C03_013E",
        "S1901_C04_001E", "S1901_C04_012E", "S1901_C04_013E"
    ]]
    acs_frames.append(df)
acs = pd.concat(acs_frames, ignore_index=True)
acs = acs[acs["GEO_ID"] != "Geography"]
acs = acs.melt(
    id_vars=["data_year", "GEO_ID", "NAME"],
    var_name="var",
    value_name="value"
)
acs[["cohort", "field"]] = acs["var"].str.extract(r"S1901_(C0\d)_(\d{3}E)")
acs = acs.pivot_table(
    index=["data_year", "GEO_ID", "NAME", "cohort"],
    columns="field",
    values="value",
    aggfunc="first"
).reset_index()
acs.columns.name = None
acs = acs.rename(columns={
    "001E": "household_count",
    "012E": "median_income",
    "013E": "mean_income",
    "NAME": "county"
})
acs["cohort"] = acs["cohort"].map({
    "C01": "household",
    "C02": "family",
    "C03": "married_couple",
    "C04": "non_family"
})
acs["GEOID"] = acs["GEO_ID"].str[-5:].str.zfill(5)
acs["state"] = acs["county"].str.extract(r",\s*(.*)$")[0]
acs["county"] = acs["county"].str.extract(r"^([^,]+?)(?: County)?(?:,|$)")[0]
for col in ["household_count", "median_income", "mean_income"]:
    acs[col] = pd.to_numeric(
        acs[col].replace(['N', '-', '(X)', 'null'], pd.NA),
        errors='coerce'
    )
acs["date"] = acs["data_year"] + "-01-01"

# ----------------------------------------------------------------------------
zillow = pd.read_csv(ZILLOW_FILE)
date_columns = [col for col in zillow.columns if re.match(r"^\d{4}-\d{2}-\d{2}$", col)]
zillow = zillow.melt(
    id_vars=[col for col in zillow.columns if col not in date_columns],
    value_vars=date_columns,
    var_name="date",
    value_name="estimated_house_price"
)
zillow["StateCodeFIPS"] = zillow["StateCodeFIPS"].astype(str).str.zfill(2)
zillow["MunicipalCodeFIPS"] = zillow["MunicipalCodeFIPS"].astype(str).str.zfill(3)
zillow["GEOID"] = (zillow["StateCodeFIPS"] + zillow["MunicipalCodeFIPS"]).str.zfill(5)
zillow["date"] = pd.to_datetime(zillow["date"]).dt.date + pd.Timedelta(days=1)
zillow["date"] = zillow["date"].astype(str)

# ----------------------------------------------------------------------------
data = pd.merge(acs, zillow, on=["GEOID", "date"], how="outer")
data = data[data["data_year"].notna()]
data["data_year"] = data["data_year"].fillna(data["date"].str[:4])
data["date"] = data["date"].fillna(data["data_year"] + "-01-01")
data = pd.merge(data, tiger, on=["GEOID", "data_year"], how="left")
data["cohort"] = data["cohort"].fillna("household")
data["State"] = data["State"].fillna(data.get("STUSPS", data.get("STATEFP")))
data["data_year"] = data["data_year"].fillna(data["date"].str[:4])
data["date"] = data["date"].fillna(data["data_year"] + "-01-01")

# ----------------------------------------------------------------------------
data_gdf = gpd.GeoDataFrame(data, geometry="geometry", crs="EPSG:4269")
data_gdf.to_parquet(UP_DIR / "us_county.parquet", index=False)
data_gdf.drop(columns="geometry").to_csv(UP_DIR / "us_county.csv", index=False)

print(f"✅ Wrote {len(data_gdf):,} rows to us_county.parquet")


    
  </code></pre>
</main>
</body>
</html>
